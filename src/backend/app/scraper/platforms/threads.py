"""Threads.com scraper implementation."""

from typing import List, Dict
from datetime import datetime
import time
import json as json_module
import asyncio

from app.scraper.base import BasePlatformScraper
from app.scraper.session_manager import SessionManager


class ThreadsScraper(BasePlatformScraper):
    """Scraper for Threads.com (Instagram Threads)."""

    def get_platform_name(self) -> str:
        """Return the platform name."""
        return "threads"

    def get_post_selectors(self) -> List[str]:
        """Return CSS selectors for Threads posts."""
        return [
            'article',
            '[role="article"]',
            'div[data-pressable-container="true"]',
            'div[class*="post"]',
            'div[class*="Post"]',
            'div[class*="thread"]',
            'div[class*="Thread"]',
            'div[role="button"]',
        ]

    async def extract_post_data(self, page, selector: str) -> List[Dict]:
        """
        Extract post data from Threads page.

        Args:
            page: Playwright page object
            selector: CSS selector for posts

        Returns:
            List of post dictionaries with text, link, likes, comments, reposts, raw_data
        """
        # First pass: extract raw data including HTML
        raw_items = await page.eval_on_selector_all(
            selector,
            """nodes => nodes.map(n => {
                const text = n.innerText;
                const link = n.querySelector('a')?.href;
                const html = n.innerHTML;

                // Extract all standalone numbers from text (engagement metrics)
                const textLines = text.split('\\n').filter(line => line.trim());
                const numbers = textLines.filter(line => /^\\d+$/.test(line.trim())).map(n => parseInt(n));

                return {
                    text: text,
                    link: link,
                    html: html,
                    raw_numbers: numbers
                };
            })"""
        )

        # Second pass: parse engagement metrics from raw numbers
        items = []
        for raw_item in raw_items:
            item = {
                'text': raw_item['text'],
                'link': raw_item['link'],
                'likes': None,
                'comments': None,
                'reposts': None,
                'raw_data': raw_item.get('html', '')
            }

            # The last 3-4 numbers in text are usually: likes, comments, reposts, (shares/other)
            numbers = raw_item.get('raw_numbers', [])
            if len(numbers) >= 3:
                # Last numbers are usually engagement metrics
                item['likes'] = numbers[-4] if len(numbers) >= 4 else numbers[-3]
                item['comments'] = numbers[-3] if len(numbers) >= 4 else numbers[-2]
                item['reposts'] = numbers[-2] if len(numbers) >= 4 else numbers[-1]

            items.append(item)

        return items

    async def scrape(self) -> Dict:
        """
        Scrape posts from a Threads profile.

        Returns:
            Dictionary containing:
                - scraped_at: Timestamp
                - url: Profile URL
                - platform: Platform name
                - user_id: User identifier
                - total_items: Number of posts scraped
                - post_limit: Post limit (if set)
                - time_limit: Time limit (if set)
                - items: List of post data
        """
        self.start_time = time.time()

        # Initialize session manager
        session_mgr = SessionManager()

        # Load browser session (returns tuple of playwright instance, context, and session_id)
        playwright, context, session_id = await session_mgr.load_session(self.user_id, headless=self.headless)
        page = context.pages[0] if context.pages else await context.new_page()

        try:
            # Navigate to profile
            print(f"ðŸŒ Navigating to: {self.url}")
            await page.goto(self.url, wait_until="domcontentloaded")
            print("â³ Waiting for page to load...")
            await asyncio.sleep(8)

            # Scroll a bit to trigger lazy loading
            await page.evaluate("window.scrollTo(0, 500)")
            await asyncio.sleep(2)

            # Find post selector
            print("ðŸ” Detecting post selector...")
            selector = await self.find_post_selector(page)

            if not selector:
                print("âŒ Could not find posts selector!")
                await context.close()
                await playwright.stop()
                session_mgr.unregister_session(session_id)
                return {
                    'error': 'No posts found',
                    'scraped_at': datetime.now().strftime("%Y%m%d_%H%M%S"),
                    'url': self.url,
                    'platform': self.get_platform_name(),
                    'user_id': self.user_id
                }

            initial_count = await page.evaluate(f'document.querySelectorAll({json_module.dumps(selector)}).length')
            print(f"âœ… Found {initial_count} posts using selector: {selector}")

            # Scroll to load more posts
            limits_desc = []
            if self.post_limit:
                limits_desc.append(f"target: {self.post_limit} posts")
            if self.time_limit:
                limits_desc.append(f"time limit: {self.time_limit}s")

            limit_str = ", ".join(limits_desc) if limits_desc else "no limit"
            print(f"\nðŸš€ Scrolling to load posts ({limit_str})...")

            final_count = await self.scroll_and_load(page, selector, max_scrolls=500)

            # Extract post data
            print(f"\nðŸ” Extracting {final_count} posts...")
            items = await self.extract_post_data(page, selector)

            # Apply post limit if needed
            if self.post_limit and len(items) > self.post_limit:
                items = items[:self.post_limit]

            print(f"âœ… Scraped {len(items)} items")

            # Calculate elapsed time
            elapsed_time = time.time() - self.start_time

            # Build result
            result = {
                'scraped_at': datetime.now().strftime("%Y%m%d_%H%M%S"),
                'url': self.url,
                'platform': self.get_platform_name(),
                'user_id': self.user_id,
                'total_items': len(items),
                'post_limit': self.post_limit,
                'time_limit': self.time_limit,
                'elapsed_time': round(elapsed_time, 2),
                'selector_used': selector,
                'items': items
            }

            return result

        finally:
            # Close context and playwright instance to ensure session data is persisted
            await context.close()
            await playwright.stop()
            session_mgr.unregister_session(session_id)
